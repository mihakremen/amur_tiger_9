{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"5fe10bf018ef3e697f9035d60bf60847932a12bface18908407fd371fe880db9"}},"colab":{"provenance":[],"gpuType":"T4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3ce9db5e3b1146039769ac19f1fe9875":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9161c3730139483b9fa58c2e47bcd5f4","IPY_MODEL_35a84009b3634c2592958d69865b8205","IPY_MODEL_2736fc3008ad44a18d8324816db1e393"],"layout":"IPY_MODEL_831005d1fe134aa684d1562944b942ce"}},"9161c3730139483b9fa58c2e47bcd5f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d87ffc6855414ec9b06ac88bd23e131d","placeholder":"â€‹","style":"IPY_MODEL_d60316a247364cd380dcb1a0aba1cbe7","value":"Resolving data files: 100%"}},"35a84009b3634c2592958d69865b8205":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b243562d2b9a4aa7965dc1a2fdf68db0","max":3048,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d2fb1c4521404f88948bf7ab38e69081","value":3048}},"2736fc3008ad44a18d8324816db1e393":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d789b2c6eff94bcb9d216208f93b4307","placeholder":"â€‹","style":"IPY_MODEL_e56dc80812c847e2903f350171938198","value":" 3048/3048 [00:00&lt;00:00, 5349.38it/s]"}},"831005d1fe134aa684d1562944b942ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d87ffc6855414ec9b06ac88bd23e131d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d60316a247364cd380dcb1a0aba1cbe7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b243562d2b9a4aa7965dc1a2fdf68db0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2fb1c4521404f88948bf7ab38e69081":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d789b2c6eff94bcb9d216208f93b4307":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e56dc80812c847e2903f350171938198":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eaac7a930e754eceb984b790a6e05247":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f082ac7a15e44d3f954767c85e19e4b6","IPY_MODEL_fb19de8f0e6b46b0bc7b1dbd45511110","IPY_MODEL_df9d78e2302145b8be9fd7f53a760d21"],"layout":"IPY_MODEL_a027d3f0a7e341269947d40974a19276"}},"f082ac7a15e44d3f954767c85e19e4b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99f59ff5dcb24406a69abc3aa532677c","placeholder":"â€‹","style":"IPY_MODEL_3a8cd816022440c0a0ab8a1c16b7492e","value":"Resolving data files: 100%"}},"fb19de8f0e6b46b0bc7b1dbd45511110":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_462beca39eee4eea88fde918b3759e15","max":1195,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4015f67ea6d148ed8ac494f0a2ba3f4b","value":1195}},"df9d78e2302145b8be9fd7f53a760d21":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b6d9b0c75304fb1bcb78c20bb1830c5","placeholder":"â€‹","style":"IPY_MODEL_8d8d1ab0c6ff44758bd0c4f4b4f59be9","value":" 1195/1195 [00:00&lt;00:00, 25966.01it/s]"}},"a027d3f0a7e341269947d40974a19276":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99f59ff5dcb24406a69abc3aa532677c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a8cd816022440c0a0ab8a1c16b7492e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"462beca39eee4eea88fde918b3759e15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4015f67ea6d148ed8ac494f0a2ba3f4b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1b6d9b0c75304fb1bcb78c20bb1830c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d8d1ab0c6ff44758bd0c4f4b4f59be9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/search/image/image-retrieval-ebook/vision-transformers/vit.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/search/image/image-retrieval-ebook/vision-transformers/vit.ipynb)","metadata":{"id":"0DM2nVwD1IHY"}},{"cell_type":"markdown","source":"# Vision Transformers (ViT) Walkthrough","metadata":{"id":"slUnn8PX1IHb"}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"!pip install datasets transformers torch","metadata":{"id":"hMK3INR81IHc","outputId":"e9a033a2-2f5e-4aa1-f220-2988a3e235de","execution":{"iopub.status.busy":"2023-11-22T13:04:05.690904Z","iopub.execute_input":"2023-11-22T13:04:05.691269Z","iopub.status.idle":"2023-11-22T13:04:19.272192Z","shell.execute_reply.started":"2023-11-22T13:04:05.691240Z","shell.execute_reply":"2023-11-22T13:04:19.271001Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.35.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.10.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.17.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -r ","metadata":{"execution":{"iopub.status.busy":"2023-11-22T08:33:29.586832Z","iopub.execute_input":"2023-11-22T08:33:29.587582Z","iopub.status.idle":"2023-11-22T08:33:30.769046Z","shell.execute_reply.started":"2023-11-22T08:33:29.587540Z","shell.execute_reply":"2023-11-22T08:33:30.767719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install roboflow\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"TR2n5JTcwaRKLAU3sNDd\")\nproject = rf.workspace(\"mike-caulfild\").project(\"chtozalevetottigr\")\ndataset = project.version(4).download(\"folder\")","metadata":{"id":"RTWb8u3S4xyK","outputId":"d61d9fae-9120-459b-da9f-6514c8b04274","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image, ImageOps\nfrom tqdm.auto import tqdm\nimport albumentations as A\n\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import datasets, models, transforms\n","metadata":{"id":"mIXUXFGd4sxJ","execution":{"iopub.status.busy":"2023-11-22T13:04:19.274100Z","iopub.execute_input":"2023-11-22T13:04:19.274432Z","iopub.status.idle":"2023-11-22T13:04:22.582556Z","shell.execute_reply.started":"2023-11-22T13:04:19.274402Z","shell.execute_reply":"2023-11-22T13:04:22.581658Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset_train = load_dataset('/kaggle/working/ChtoZaLevEtotTigr-4/train',\n    split='train', # training dataset\n    ignore_verifications=False  # set to True if seeing splits Error\n)\ndataset_val = load_dataset('/kaggle/working/ChtoZaLevEtotTigr-4/valid',\n    split='train', # training dataset\n    ignore_verifications=False  # set to True if seeing splits Error)\n)\n# dataset_test = load_dataset('/kaggle/working/ChtoZaLevEtotTigr-4/test',\n#     split='train', # training dataset\n#     ignore_verifications=False  # set to True if seeing splits Error)\n# )","metadata":{"id":"o73IxMIL9Sar","outputId":"ceca0e10-8e2f-48fb-b041-ef67d16a34e5","execution":{"iopub.status.busy":"2023-11-22T11:27:13.144911Z","iopub.execute_input":"2023-11-22T11:27:13.145310Z","iopub.status.idle":"2023-11-22T11:27:16.801799Z","shell.execute_reply.started":"2023-11-22T11:27:13.145277Z","shell.execute_reply":"2023-11-22T11:27:16.800974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train[643]","metadata":{"id":"fFB_sjXl1gbS","outputId":"470eaf7d-a585-4b8d-c975-caee2aa72600","execution":{"iopub.status.busy":"2023-11-22T11:27:22.045599Z","iopub.execute_input":"2023-11-22T11:27:22.045960Z","iopub.status.idle":"2023-11-22T11:27:22.071869Z","shell.execute_reply.started":"2023-11-22T11:27:22.045931Z","shell.execute_reply":"2023-11-22T11:27:22.070835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check how many labels/number of classes\nnum_classes = len(set(dataset_train['label']))\nlabels = dataset_train.features['label']\nnum_classes, labels","metadata":{"id":"4A94eieh1IHm","outputId":"6603bf8a-a3b5-4a9c-a62e-5197d08f4a03","execution":{"iopub.status.busy":"2023-11-22T08:35:01.482421Z","iopub.execute_input":"2023-11-22T08:35:01.483306Z","iopub.status.idle":"2023-11-22T08:35:01.492549Z","shell.execute_reply.started":"2023-11-22T08:35:01.483273Z","shell.execute_reply":"2023-11-22T08:35:01.491580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Those are PIL images with $3$ color channels, and $32x32$ pixels resolution. Let's have a look at the first picture in the dataset.","metadata":{"id":"BGpxHuN01IHn"}},{"cell_type":"code","source":"dataset_train[0]['image']","metadata":{"id":"q92Frds01IHo","outputId":"b5364963-49c3-4606-d01a-71ee984f53bc","execution":{"iopub.status.busy":"2023-11-22T08:35:03.767703Z","iopub.execute_input":"2023-11-22T08:35:03.768132Z","iopub.status.idle":"2023-11-22T08:35:03.811247Z","shell.execute_reply.started":"2023-11-22T08:35:03.768100Z","shell.execute_reply":"2023-11-22T08:35:03.810271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('width, height',dataset_train[0]['image'].size, 'FORMAT', dataset_train[0]['image'].format, 'MODE', dataset_train[0]['image'].mode)\n","metadata":{"id":"cNBfzfoObxc7","outputId":"08e214b3-6da1-4ae3-b616-4bba0782e3f7","scrolled":true,"execution":{"iopub.status.busy":"2023-11-22T08:35:06.170540Z","iopub.execute_input":"2023-11-22T08:35:06.171491Z","iopub.status.idle":"2023-11-22T08:35:06.180401Z","shell.execute_reply.started":"2023-11-22T08:35:06.171452Z","shell.execute_reply":"2023-11-22T08:35:06.179302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train[0]['label'], labels.names[dataset_train[0]['label']]","metadata":{"id":"btJZgg2t1IHo","outputId":"b9d3f4e4-ab6d-438a-9736-8fa8b739443f","execution":{"iopub.status.busy":"2023-11-22T08:35:07.761301Z","iopub.execute_input":"2023-11-22T08:35:07.761802Z","iopub.status.idle":"2023-11-22T08:35:07.772046Z","shell.execute_reply.started":"2023-11-22T08:35:07.761762Z","shell.execute_reply":"2023-11-22T08:35:07.770961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading ViT Feature Extractor","metadata":{"id":"lD2I2d3g1IHo"}},{"cell_type":"markdown","source":"We use `google/vit-base-patch16-224-in21k` model from the Hugging Face Hub.","metadata":{"id":"y8BIHPgr1IHp"}},{"cell_type":"markdown","source":"The model is named as so as it refers to base-sized architecture with patch resolution of 16x16 and fine-tuning resolution of 224x224.  ","metadata":{"id":"LTVrnp861IHp"}},{"cell_type":"code","source":"from transformers import ViTImageProcessor\n\n# import model\nmodel_id = 'google/vit-base-patch16-224-in21k'\nfeature_extractor = ViTImageProcessor.from_pretrained(\n    model_id\n)","metadata":{"id":"wIXoODNc1IHp","execution":{"iopub.status.busy":"2023-11-22T13:04:22.583781Z","iopub.execute_input":"2023-11-22T13:04:22.584192Z","iopub.status.idle":"2023-11-22T13:04:35.874212Z","shell.execute_reply.started":"2023-11-22T13:04:22.584153Z","shell.execute_reply":"2023-11-22T13:04:35.873274Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)rocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd0dfd785c62431c83ba33fe26cd908d"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see the feature extractor configuration by printing it","metadata":{"id":"hG6O413i1IHp"}},{"cell_type":"markdown","source":"If we consider the first image, i.e., the airplane shown above, we can see the resulting tensor after passing the image through the feature extractor.","metadata":{"id":"W07gDA5r1IHq"}},{"cell_type":"code","source":"# load in relevant libraries, and alias where appropriate\nimport torch\n\n# device will determine whether to run the training on GPU or CPU.\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"id":"rcBeAXTD1IHr","outputId":"02543adf-fcb7-4c0d-c358-b6488c5a4f37","execution":{"iopub.status.busy":"2023-11-22T13:04:35.876867Z","iopub.execute_input":"2023-11-22T13:04:35.877829Z","iopub.status.idle":"2023-11-22T13:04:35.957464Z","shell.execute_reply.started":"2023-11-22T13:04:35.877792Z","shell.execute_reply":"2023-11-22T13:04:35.956497Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess(batch):\n    # take a list of PIL images and turn them to pixel values\n    inputs = feature_extractor(\n        batch['image'],\n        return_tensors='pt'\n    )\n    # include the labels\n    inputs['label'] = batch['label']\n    return inputs","metadata":{"id":"lZ5XKRX21IHs","execution":{"iopub.status.busy":"2023-11-22T11:27:43.326893Z","iopub.execute_input":"2023-11-22T11:27:43.327298Z","iopub.status.idle":"2023-11-22T11:27:43.335628Z","shell.execute_reply.started":"2023-11-22T11:27:43.327259Z","shell.execute_reply":"2023-11-22T11:27:43.334682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can apply this to both the training and testing dataset.","metadata":{"id":"B5Vmda1N1IHs"}},{"cell_type":"code","source":"# transform the training dataset\nprepared_train = dataset_train.with_transform(preprocess)\nprepared_val = dataset_val.with_transform(preprocess)\n# prepared_test = dataset_test.with_transform(preprocess)","metadata":{"id":"WS99AoOO1IHs","execution":{"iopub.status.busy":"2023-11-22T11:27:43.336934Z","iopub.execute_input":"2023-11-22T11:27:43.337249Z","iopub.status.idle":"2023-11-22T11:27:43.382571Z","shell.execute_reply.started":"2023-11-22T11:27:43.337221Z","shell.execute_reply":"2023-11-22T11:27:43.381498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prepared_train","metadata":{"id":"e7upv8oqTdwQ","outputId":"0c7af197-d747-442e-d3ea-f189034fdde9","execution":{"iopub.status.busy":"2023-11-22T11:27:43.383778Z","iopub.execute_input":"2023-11-22T11:27:43.384274Z","iopub.status.idle":"2023-11-22T11:27:43.403159Z","shell.execute_reply.started":"2023-11-22T11:27:43.384225Z","shell.execute_reply":"2023-11-22T11:27:43.402021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, whenever you get an example from the dataset, the transform will be applied in real time (on both samples and slices).","metadata":{"id":"uf-ZQlvM1IHs"}},{"cell_type":"markdown","source":"### Model Fine-Tuning","metadata":{"id":"n3477h2n1IHs"}},{"cell_type":"markdown","source":"In this section, we are going to build the Trainer, which is a feature-complete training and eval loop for PyTorch, optimized for HuggingFace ðŸ¤— Transformers.\n\nWe need to define all of the arguments that it will include:\n* training and testing dataset\n* feature extractor\n* model\n* collate function\n* evaluation metric\n* ... other training arguments.","metadata":{"id":"B2jEhVAa1IHt"}},{"cell_type":"markdown","source":"The collate function is useful when dealing with lots of data. Batches are lists of dictionaries, so collate will help us create batch tensors.","metadata":{"id":"cGQPYzmK1IHt"}},{"cell_type":"code","source":"def collate_fn(batch):\n    return {\n        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n        'labels': torch.tensor([x['label'] for x in batch])\n    }","metadata":{"id":"OEVXUri61IHt","execution":{"iopub.status.busy":"2023-11-22T11:27:43.404669Z","iopub.execute_input":"2023-11-22T11:27:43.405095Z","iopub.status.idle":"2023-11-22T11:27:43.414499Z","shell.execute_reply.started":"2023-11-22T11:27:43.405057Z","shell.execute_reply":"2023-11-22T11:27:43.413563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now define the evaluation metric we are going to use to compare prediction with actual labels. We will use the *accuracy evaluation metric*.\n\nAccuracy is defined as the proportion of correct predictions (True Positive ($TP$) and True Negative ($TN$)) among the total number of cases processed ($TP$, $TN$, False Positive ($FP$), and False Negative ($FN$)).\n\n$$Accuracy = \\frac{(TP + TN)}{(TP + TN + FP + FN)}$$    \n\nBelow, we are using accuracy within the ```compute_metrics``` function.","metadata":{"id":"EcGIL-351IHu"}},{"cell_type":"code","source":"import numpy as np\nfrom datasets import load_metric\n\n# f1 metric\nmetric = load_metric(\"f1\")\ndef compute_metrics(p):\n    return metric.compute(\n        predictions=np.argmax(p.predictions, axis=1),\n        references=p.label_ids,\n        average='weighted'\n    )","metadata":{"id":"ih484bKP1IHu","execution":{"iopub.status.busy":"2023-11-22T11:27:43.415902Z","iopub.execute_input":"2023-11-22T11:27:43.416259Z","iopub.status.idle":"2023-11-22T11:27:43.853918Z","shell.execute_reply.started":"2023-11-22T11:27:43.416228Z","shell.execute_reply":"2023-11-22T11:27:43.852986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The last thing consists of defining ```TrainingArguments```.\n\nMost of these are pretty self-explanatory, but one that is quite important here is ```remove_unused_columns=False```. This one will drop any features not used by the model's call function. By default it's True because usually it's ideal to drop unused feature columns, making it easier to unpack inputs into the model's call function. But, in our case, we need the unused features ('image' in particular) in order to create 'pixel_values'.\n\nWe have chosen a batch size equal to 16, 100 evaluation steps, and a learning rate of $2e^{-4}$.","metadata":{"id":"AI2CXiU71IHu"}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n  output_dir=\"/kaggle/working/\",\n  per_device_train_batch_size=32,\n  evaluation_strategy=\"epoch\",\n  logging_strategy = \"epoch\",\n  save_strategy =  \"epoch\",\n  num_train_epochs=30,\n#   save_steps=100,\n#   eval_steps=100,\n#   logging_steps=10,\n  learning_rate=2e-4,\n  save_total_limit=2,\n  remove_unused_columns=False,\n  push_to_hub=False,\n  load_best_model_at_end=True,\n)","metadata":{"id":"sVlvPiq31IHu","execution":{"iopub.status.busy":"2023-11-22T11:27:43.855210Z","iopub.execute_input":"2023-11-22T11:27:43.855548Z","iopub.status.idle":"2023-11-22T11:27:44.099500Z","shell.execute_reply.started":"2023-11-22T11:27:43.855515Z","shell.execute_reply":"2023-11-22T11:27:44.098378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now load the pre-trained model. We'll add ```num_labels``` on init so the model creates a classification head with the right number of units.","metadata":{"id":"DJTlqD7k1IHv"}},{"cell_type":"code","source":"from transformers import ViTForImageClassification\n\nlabels = {0:'front', 1:'left', 2:'other', 3:'right'}\n\nmodel = ViTForImageClassification.from_pretrained(\n    model_id,  # classification head\n    num_labels=len(labels)\n)","metadata":{"id":"BctOMKPh1IHv","outputId":"178b2ed4-de83-4d37-9289-fa4aeaa32178","execution":{"iopub.status.busy":"2023-11-22T13:04:35.958990Z","iopub.execute_input":"2023-11-22T13:04:35.959815Z","iopub.status.idle":"2023-11-22T13:04:39.763985Z","shell.execute_reply.started":"2023-11-22T13:04:35.959785Z","shell.execute_reply":"2023-11-22T13:04:39.763194Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22f4aca40b1e4e5898c95a4c0594fbac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94ce70467267441cba4a40f9b949ec8e"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"len(labels)","metadata":{"id":"oKVti8Rrgcgr","outputId":"2243ef19-e381-4b5b-c631-ae7bbc5f0830","execution":{"iopub.status.busy":"2023-11-22T11:27:54.643625Z","iopub.execute_input":"2023-11-22T11:27:54.644383Z","iopub.status.idle":"2023-11-22T11:27:54.650271Z","shell.execute_reply.started":"2023-11-22T11:27:54.644348Z","shell.execute_reply":"2023-11-22T11:27:54.649198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)","metadata":{"id":"TneAF1xr1IHv","outputId":"1ba5bf73-edbd-4764-8e18-f53d88d8de0d","execution":{"iopub.status.busy":"2023-11-22T13:04:39.765144Z","iopub.execute_input":"2023-11-22T13:04:39.765537Z","iopub.status.idle":"2023-11-22T13:04:45.185853Z","shell.execute_reply.started":"2023-11-22T13:04:39.765503Z","shell.execute_reply":"2023-11-22T13:04:45.184785Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"ViTForImageClassification(\n  (vit): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  )\n  (classifier): Linear(in_features=768, out_features=4, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see the characteristics of our model.","metadata":{"id":"8xAaOEt-1IHw"}},{"cell_type":"markdown","source":"Now, all instances can be passed to ```Trainer```.","metadata":{"id":"-A-U0E7_1IHw"}},{"cell_type":"code","source":"!rm -r /kaggle/working/runs /kaggle/working/checkpoint-144 /kaggle/working/checkpoint-288 /kaggle/working/trainer_state.json /kaggle/working/training_args.bin /kaggle/working/train_results.json /kaggle/working/all_results.json /kaggle/working/preprocessor_config.json /kaggle/working/eval_results.json /kaggle/working/config.json /kaggle/working/model.safetensors /kaggle/working/state.db /kaggle/working/ChtoZaLevEtotTigr-10 /kaggle/working/wandb /kaggle/working/ChtoZaLevEtotTigr-6","metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:29:10.983768Z","iopub.execute_input":"2023-11-22T11:29:10.984962Z","iopub.status.idle":"2023-11-22T11:29:12.230855Z","shell.execute_reply.started":"2023-11-22T11:29:10.984912Z","shell.execute_reply":"2023-11-22T11:29:12.229466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import EarlyStoppingCallback","metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:29:23.996608Z","iopub.execute_input":"2023-11-22T11:29:23.997018Z","iopub.status.idle":"2023-11-22T11:29:24.002108Z","shell.execute_reply.started":"2023-11-22T11:29:23.996983Z","shell.execute_reply":"2023-11-22T11:29:24.001112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = EarlyStoppingCallback(early_stopping_patience=3)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:29:25.370314Z","iopub.execute_input":"2023-11-22T11:29:25.371222Z","iopub.status.idle":"2023-11-22T11:29:25.375710Z","shell.execute_reply.started":"2023-11-22T11:29:25.371184Z","shell.execute_reply":"2023-11-22T11:29:25.374682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n    train_dataset=prepared_train,\n    eval_dataset=prepared_val,\n    callbacks = [early_stopping],\n    tokenizer=feature_extractor\n)","metadata":{"id":"WEz7ZeQ41IH1","execution":{"iopub.status.busy":"2023-11-22T11:29:28.093566Z","iopub.execute_input":"2023-11-22T11:29:28.094050Z","iopub.status.idle":"2023-11-22T11:29:29.079248Z","shell.execute_reply.started":"2023-11-22T11:29:28.094017Z","shell.execute_reply":"2023-11-22T11:29:29.078208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can save our trained model.","metadata":{"id":"UVSAUbFT1IH1"}},{"cell_type":"code","source":"!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:30:11.175551Z","iopub.execute_input":"2023-11-22T11:30:11.176245Z","iopub.status.idle":"2023-11-22T11:30:23.789854Z","shell.execute_reply.started":"2023-11-22T11:30:11.176209Z","shell.execute_reply":"2023-11-22T11:30:23.788876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:31:09.231218Z","iopub.execute_input":"2023-11-22T11:31:09.231991Z","iopub.status.idle":"2023-11-22T11:31:30.866820Z","shell.execute_reply.started":"2023-11-22T11:31:09.231955Z","shell.execute_reply":"2023-11-22T11:31:30.865861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wandb login --relogin","metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:33:21.721885Z","iopub.execute_input":"2023-11-22T11:33:21.722758Z","iopub.status.idle":"2023-11-22T11:33:53.595461Z","shell.execute_reply.started":"2023-11-22T11:33:21.722722Z","shell.execute_reply":"2023-11-22T11:33:53.594364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.init()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T11:36:21.561219Z","iopub.execute_input":"2023-11-22T11:36:21.561879Z","iopub.status.idle":"2023-11-22T11:37:22.244325Z","shell.execute_reply.started":"2023-11-22T11:36:21.561847Z","shell.execute_reply":"2023-11-22T11:37:22.243257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_results = trainer.train()\n# save tokenizer with the model\ntrainer.save_model()\ntrainer.log_metrics(\"train\", train_results.metrics)\ntrainer.save_metrics(\"train\", train_results.metrics)\n# save the trainer state\ntrainer.save_state()","metadata":{"id":"6rtmSypT1IH1","outputId":"f7b3f159-ceaf-4942-d47a-0a3461ee404b","execution":{"iopub.status.busy":"2023-11-22T11:37:59.468412Z","iopub.execute_input":"2023-11-22T11:37:59.469115Z","iopub.status.idle":"2023-11-22T11:45:53.258736Z","shell.execute_reply.started":"2023-11-22T11:37:59.469079Z","shell.execute_reply":"2023-11-22T11:45:53.257756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r /content/model.zip /content/model/model_vit","metadata":{"id":"nBXGz8tP7TF6","outputId":"42c67f14-148f-4c53-dc67-3a4d2a447699"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Evaluation","metadata":{"id":"Ikzt_I2O1IH2"}},{"cell_type":"markdown","source":"We can now evaluate our model using the accuracy metric defined above...","metadata":{"id":"Sr7iMCD71IH2"}},{"cell_type":"code","source":"metrics = trainer.evaluate(prepared_test)\ntrainer.log_metrics(\"eval\", metrics)\ntrainer.save_metrics(\"eval\", metrics)","metadata":{"id":"oSybjQHt1IH2","execution":{"iopub.status.busy":"2023-11-22T10:06:30.345816Z","iopub.execute_input":"2023-11-22T10:06:30.346618Z","iopub.status.idle":"2023-11-22T10:06:38.629549Z","shell.execute_reply.started":"2023-11-22T10:06:30.346585Z","shell.execute_reply":"2023-11-22T10:06:38.628360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model accuracy is pretty good. Let's have a look to an example. We can pick the first image in our testing dataset and see if the predicted label is correct.","metadata":{"id":"77p6CRN_1IH2"}},{"cell_type":"code","source":"import json #ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡Ð¸Ð»Ð¸ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÑƒ\n \nwith open('/kaggle/working/trainer_state.json', 'r', encoding='utf-8') as f: #Ð¾Ñ‚ÐºÑ€Ñ‹Ð»Ð¸ Ñ„Ð°Ð¹Ð»\n    text = json.load(f) #Ð·Ð°Ð³Ð½Ð°Ð»Ð¸ Ð²ÑÐµ Ð¸Ð· Ñ„Ð°Ð¹Ð»Ð° Ð² Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ\n    print(text) #Ð²Ñ‹Ð²ÐµÐ»Ð¸ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð½Ð° ÑÐºÑ€Ð°Ð½","metadata":{"execution":{"iopub.status.busy":"2023-11-22T12:54:24.749521Z","iopub.execute_input":"2023-11-22T12:54:24.749963Z","iopub.status.idle":"2023-11-22T12:54:24.757512Z","shell.execute_reply.started":"2023-11-22T12:54:24.749927Z","shell.execute_reply":"2023-11-22T12:54:24.756256Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{'best_metric': 0.27228328585624695, 'best_model_checkpoint': '/kaggle/working/checkpoint-48', 'epoch': 4.0, 'eval_steps': 500, 'global_step': 192, 'is_hyper_param_search': False, 'is_local_process_zero': True, 'is_world_process_zero': True, 'log_history': [{'epoch': 1.0, 'learning_rate': 0.00019333333333333333, 'loss': 0.2642, 'step': 48}, {'epoch': 1.0, 'eval_f1': 0.9012113001529236, 'eval_loss': 0.27228328585624695, 'eval_runtime': 23.1095, 'eval_samples_per_second': 51.71, 'eval_steps_per_second': 3.245, 'step': 48}, {'epoch': 2.0, 'learning_rate': 0.0001866666666666667, 'loss': 0.1556, 'step': 96}, {'epoch': 2.0, 'eval_f1': 0.8919466169890291, 'eval_loss': 0.305475652217865, 'eval_runtime': 22.8647, 'eval_samples_per_second': 52.264, 'eval_steps_per_second': 3.28, 'step': 96}, {'epoch': 3.0, 'learning_rate': 0.00018, 'loss': 0.093, 'step': 144}, {'epoch': 3.0, 'eval_f1': 0.8952650762052572, 'eval_loss': 0.3232346177101135, 'eval_runtime': 23.2968, 'eval_samples_per_second': 51.295, 'eval_steps_per_second': 3.219, 'step': 144}, {'epoch': 4.0, 'learning_rate': 0.00017333333333333334, 'loss': 0.0703, 'step': 192}, {'epoch': 4.0, 'eval_f1': 0.8983309455870018, 'eval_loss': 0.342627614736557, 'eval_runtime': 22.9691, 'eval_samples_per_second': 52.026, 'eval_steps_per_second': 3.265, 'step': 192}, {'epoch': 4.0, 'step': 192, 'total_flos': 9.447992729516114e+17, 'train_loss': 0.1458008922636509, 'train_runtime': 472.4792, 'train_samples_per_second': 193.532, 'train_steps_per_second': 3.048}], 'logging_steps': 500, 'max_steps': 1440, 'num_train_epochs': 30, 'save_steps': 500, 'total_flos': 9.447992729516114e+17, 'trial_name': None, 'trial_params': None}\n","output_type":"stream"}]},{"cell_type":"code","source":"# show the first image of the testing dataset\nimage = dataset_test[\"image\"][0].resize((200,200))\nimage","metadata":{"id":"e8-Bpg9X1IH2","outputId":"e82db65a-5443-4ff2-f3c0-a1b7bcd8c243","execution":{"iopub.status.busy":"2023-11-22T07:28:24.051075Z","iopub.execute_input":"2023-11-22T07:28:24.051788Z","iopub.status.idle":"2023-11-22T07:28:24.684691Z","shell.execute_reply.started":"2023-11-22T07:28:24.051755Z","shell.execute_reply":"2023-11-22T07:28:24.683405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The image is not very clear, even when resized. Let's extract the actual label.","metadata":{"id":"WFuc5Oty1IH2"}},{"cell_type":"code","source":"# extract the actual label of the first image of the testing dataset\nactual_label = dataset_test[\"label\"][0]\n\nlabels = dataset_test.features['label']\nactual_label, labels.names[actual_label]\n","metadata":{"id":"bG19Zds71IH3","outputId":"8c3405a7-d13c-4e32-ca9e-bef5d94ef2bb","execution":{"iopub.status.busy":"2023-11-22T07:28:31.016644Z","iopub.execute_input":"2023-11-22T07:28:31.017017Z","iopub.status.idle":"2023-11-22T07:28:31.026627Z","shell.execute_reply.started":"2023-11-22T07:28:31.016985Z","shell.execute_reply":"2023-11-22T07:28:31.025186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like the image represents a cat. Let's now see what our model has predicted. Given we saved it on the HuggingFace Hub, we first need to import it. We can use ViTForImageClassification and ViTFeatureExtractor to import the model and extract its features. We would need the predicted pixel values \"pt\".","metadata":{"id":"a6z1Gs-a1IH3"}},{"cell_type":"code","source":"import urllib.request\nfrom PIL import Image","metadata":{"id":"ddZ36vkybxc5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_url = 'https://img5.goodfon.com/original/2304x1536/f/46/tigr-progulka-les-tuman.jpg'\n\nurllib.request.urlretrieve(image_url,\"image.jpg\")","metadata":{"id":"mqiZ3q1pbxc6","outputId":"a33b2868-af12-4090-b67f-e44ff7b6264b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = Image.open(\"image.jpg\")\n\nimg.show();","metadata":{"id":"yaFwUz5Cbxc7","scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img","metadata":{"id":"-2nVpbYw-Mpr","outputId":"f359514c-9026-4b04-a199-8d4a6299403d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import ViTForImageClassification, ViTFeatureExtractor\n\n# import our fine-tuned model\nmodel_name_or_path = '/kaggle/working/checkpoint-2400'\nmodel_finetuned = ViTForImageClassification.from_pretrained(model_name_or_path)\n# import features\nfeature_extractor_finetuned = ViTImageProcessor.from_pretrained(model_name_or_path)","metadata":{"id":"N8By3b_k1IH3","execution":{"iopub.status.busy":"2023-11-22T07:28:53.255785Z","iopub.execute_input":"2023-11-22T07:28:53.256168Z","iopub.status.idle":"2023-11-22T07:28:54.056834Z","shell.execute_reply.started":"2023-11-22T07:28:53.256135Z","shell.execute_reply":"2023-11-22T07:28:54.055674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = feature_extractor_finetuned(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model_finetuned(**inputs).logits","metadata":{"id":"GQ-GNzt71IH3","execution":{"iopub.status.busy":"2023-11-22T07:29:13.421862Z","iopub.execute_input":"2023-11-22T07:29:13.422836Z","iopub.status.idle":"2023-11-22T07:29:13.769078Z","shell.execute_reply.started":"2023-11-22T07:29:13.422799Z","shell.execute_reply":"2023-11-22T07:29:13.767753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_label = logits.argmax(-1).item()\nprint(predicted_label)\nlabels = dataset_test.features['label']\nlabels.names[predicted_label]","metadata":{"id":"xLjs6xO91IH4","outputId":"91cfec94-a01f-4493-ffa0-fd9bde090968","execution":{"iopub.status.busy":"2023-11-22T07:29:16.390809Z","iopub.execute_input":"2023-11-22T07:29:16.391455Z","iopub.status.idle":"2023-11-22T07:29:16.405392Z","shell.execute_reply.started":"2023-11-22T07:29:16.391413Z","shell.execute_reply":"2023-11-22T07:29:16.403892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now see what is our predicted label. Do extract it, we can use the argmax function.","metadata":{"id":"koeHs0Ob1IH3"}},{"cell_type":"markdown","source":"And the answer is cat. Which is what we would expect.","metadata":{"id":"L_8uF3l61IH4"}},{"cell_type":"markdown","source":"## References\n\n[Article](https://pinecone.io/learn/vision-transformers/)\n\n[1] Dosovitskiy et al., [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929), 2021, CV.\n\n[2] Vaswani et al., [Attention Is All You Need](https://arxiv.org/abs/1706.03762), 2017.\n\n[3] Saeed M., [A Gentle Introduction to Positional Encoding in Transformer Models, Part 1](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/), 2022, Attention, Machine Learning Mastery.","metadata":{"id":"0-BJoJAh1IH4"}}]}